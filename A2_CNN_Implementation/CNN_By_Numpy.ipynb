{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Regularization functions\n",
    "def l1_regularization(weights, alpha):\n",
    "    return alpha * np.sum(np.abs(weights))\n",
    "\n",
    "def l2_regularization(weights, alpha):\n",
    "    return alpha * np.sum(weights ** 2)\n",
    "\n",
    "# Loss function (example)\n",
    "def categorical_crossentropy(predictions, targets):\n",
    "    epsilon = 1e-12\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce_loss = -np.sum(targets * np.log(predictions + 1e-9)) / N\n",
    "    return ce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self, filters, kernel_size, stride=1, padding=0):\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "        self.input = None\n",
    "\n",
    "    def initialize_parameters(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.weights = np.random.randn(self.filters, input_shape[1], self.kernel_size, self.kernel_size) * np.sqrt(2.0 / (input_shape[1] * self.kernel_size * self.kernel_size))\n",
    "        self.bias = np.zeros((self.filters, 1))\n",
    "        self.grad_weights = np.zeros_like(self.weights)\n",
    "        self.grad_bias = np.zeros_like(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        batch_size, channels, height, width = input.shape\n",
    "        if self.weights is None:\n",
    "            self.initialize_parameters(input.shape)\n",
    "        padded_input = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "        height_out = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        width_out = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        output = np.zeros((batch_size, self.filters, height_out, width_out))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for f in range(self.filters):\n",
    "                for i in range(0, height_out):\n",
    "                    for j in range(0, width_out):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        patch = padded_input[b, :, h_start:h_end, w_start:w_end]\n",
    "                        output[b, f, i, j] = np.sum(patch * self.weights[f]) + self.bias[f]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output, learning_rate):\n",
    "        batch_size, _, height, width = self.input.shape\n",
    "        padded_input = np.pad(self.input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "        d_input = np.zeros_like(padded_input)\n",
    "        height_out, width_out = d_output.shape[2], d_output.shape[3]\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for f in range(self.filters):\n",
    "                for i in range(0, height_out):\n",
    "                    for j in range(0, width_out):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        patch = padded_input[b, :, h_start:h_end, w_start:w_end]\n",
    "                        self.grad_weights[f] += d_output[b, f, i, j] * patch\n",
    "                        self.grad_bias[f] += d_output[b, f, i, j]\n",
    "                        d_input[b, :, h_start:h_end, w_start:w_end] += d_output[b, f, i, j] * self.weights[f]\n",
    "\n",
    "        if self.padding != 0:\n",
    "            d_input = d_input[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "        self.weights -= learning_rate * self.grad_weights / batch_size\n",
    "        self.bias -= learning_rate * self.grad_bias / batch_size\n",
    "\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling2D:\n",
    "    def __init__(self, pool_size=(2, 2), stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.input = None\n",
    "        self.height_out = None\n",
    "        self.width_out = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        batch_size, channels, height, width = input.shape\n",
    "        self.height_out = (height - self.pool_size[0]) // self.stride + 1\n",
    "        self.width_out = (width - self.pool_size[1]) // self.stride + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, channels, self.height_out, self.width_out))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(self.height_out):\n",
    "                    for j in range(self.width_out):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.pool_size[0]\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.pool_size[1]\n",
    "                        \n",
    "                        patch = input[b, c, h_start:h_end, w_start:w_end]\n",
    "                        output[b, c, i, j] = np.max(patch)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        batch_size, channels, _, _ = self.input.shape\n",
    "        d_input = np.zeros_like(self.input)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(self.height_out):\n",
    "                    for j in range(self.width_out):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.pool_size[0]\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.pool_size[1]\n",
    "                        \n",
    "                        patch = self.input[b, c, h_start:h_end, w_start:w_end]\n",
    "                        mask = (patch == np.max(patch))\n",
    "                        d_input[b, c, h_start:h_end, w_start:w_end] += mask * d_output[b, c, i, j]\n",
    "        \n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_units, units, activation=None, weight_regularizer=None):\n",
    "        self.input_units = input_units\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "        self.input = None\n",
    "        \n",
    "        self.initialize_parameters()\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(self.input_units, self.units) * np.sqrt(2.0 / self.input_units)\n",
    "        self.bias = np.zeros((1, self.units))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        output = np.dot(input, self.weights) + self.bias\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, output)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-output))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(output)\n",
    "        elif self.activation == 'softmax':\n",
    "            exp_scores = np.exp(output)\n",
    "            return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        else:\n",
    "            return output  # Linear activation\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        batch_size = self.input.shape[0]\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            grad_activation = (self.input > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            grad_activation = sigmoid(self.input) * (1 - sigmoid(self.input))\n",
    "        elif self.activation == 'tanh':\n",
    "            grad_activation = 1 - np.tanh(self.input) ** 2\n",
    "        elif self.activation == 'softmax':\n",
    "            # Softmax gradient is handled separately in cross-entropy loss function\n",
    "            grad_activation = 1\n",
    "        else:\n",
    "            grad_activation = 1  # Linear activation\n",
    "        \n",
    "        d_input = np.dot(d_output, self.weights.T) * grad_activation\n",
    "        self.grad_weights = np.dot(self.input.T, d_output) / batch_size\n",
    "        self.grad_bias = np.mean(d_output, axis=0, keepdims=True)\n",
    "        \n",
    "        if self.weight_regularizer == 'L1':\n",
    "            self.grad_weights += np.sign(self.weights) * self.weight_regularizer_strength\n",
    "        elif self.weight_regularizer == 'L2':\n",
    "            self.grad_weights += 2 * self.weights * self.weight_regularizer_strength\n",
    "        \n",
    "        self.weights -= learning_rate * self.grad_weights\n",
    "        self.bias -= learning_rate * self.grad_bias\n",
    "        \n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = None\n",
    "        self.running_var = None\n",
    "        self.beta = None\n",
    "        self.gamma = None\n",
    "        self.grad_beta = None\n",
    "        self.grad_gamma = None\n",
    "        self.input = None\n",
    "        self.batch_size = None\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        self.gamma = np.ones((1, input_shape[0], 1, 1))\n",
    "        self.beta = np.zeros((1, input_shape[0], 1, 1))\n",
    "        self.running_mean = np.zeros((1, input_shape[0], 1, 1))\n",
    "        self.running_var = np.ones((1, input_shape[0], 1, 1))\n",
    "        \n",
    "    def forward(self, input, mode='train'):\n",
    "        self.input = input\n",
    "        self.batch_size, self.channels, self.height, self.width = input.shape\n",
    "        \n",
    "        if self.gamma is None or self.beta is None:\n",
    "            self.initialize((self.channels,))\n",
    "        \n",
    "        if mode == 'train':\n",
    "            sample_mean = np.mean(input, axis=(0, 2, 3), keepdims=True)\n",
    "            sample_var = np.var(input, axis=(0, 2, 3), keepdims=True)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * sample_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * sample_var\n",
    "        else:\n",
    "            sample_mean = self.running_mean\n",
    "            sample_var = self.running_var\n",
    "        \n",
    "        self.x_hat = (input - sample_mean) / np.sqrt(sample_var + self.epsilon)\n",
    "        output = self.gamma * self.x_hat + self.beta\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        d_gamma = np.sum(d_output * self.x_hat, axis=(0, 2, 3), keepdims=True)\n",
    "        d_beta = np.sum(d_output, axis=(0, 2, 3), keepdims=True)\n",
    "        \n",
    "        d_x_hat = d_output * self.gamma\n",
    "        N = self.batch_size * self.height * self.width\n",
    "        d_var = np.sum(d_x_hat * (self.input - self.running_mean) * (-0.5) * np.power(self.running_var + self.epsilon, -1.5), axis=(0, 2, 3), keepdims=True)\n",
    "        d_mean = np.sum(d_x_hat * (-1) / np.sqrt(self.running_var + self.epsilon), axis=(0, 2, 3), keepdims=True) + d_var * np.sum(-2 * (self.input - self.running_mean), axis=(0, 2, 3), keepdims=True) / N\n",
    "        \n",
    "        d_input = d_x_hat / np.sqrt(self.running_var + self.epsilon) + d_var * 2 * (self.input - self.running_mean) / N + d_mean / N\n",
    "        self.grad_gamma = d_gamma\n",
    "        self.grad_beta = d_beta\n",
    "        \n",
    "        self.gamma -= learning_rate * self.grad_gamma\n",
    "        self.beta -= learning_rate * self.grad_beta\n",
    "        \n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, activation_type):\n",
    "        self.activation_type = activation_type\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        \n",
    "        if self.activation_type == 'relu':\n",
    "            return np.maximum(0, input)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-input))\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return np.tanh(input)\n",
    "        else:\n",
    "            raise ValueError(f\"Activation '{self.activation_type}' is not supported.\")\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        if self.activation_type == 'relu':\n",
    "            grad = (self.input > 0).astype(float)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            grad = sigmoid(self.input) * (1 - sigmoid(self.input))\n",
    "        elif self.activation_type == 'tanh':\n",
    "            grad = 1 - np.tanh(self.input) ** 2\n",
    "        else:\n",
    "            raise ValueError(f\"Activation '{self.activation_type}' is not supported.\")\n",
    "        \n",
    "        return d_output * grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, input, mode='train'):\n",
    "        if mode == 'train':\n",
    "            self.mask = (np.random.rand(*input.shape) >= self.dropout_rate) / (1 - self.dropout_rate)\n",
    "            output = input * self.mask\n",
    "        else:\n",
    "            output = input\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        d_input = d_output * self.mask\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def forward(self, input):\n",
    "        self.input_shape = input.shape\n",
    "        batch_size = input.shape[0]\n",
    "        return input.reshape(batch_size, -1)\n",
    "    \n",
    "    def backward(self, d_output, learning_rate):\n",
    "        return d_output.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update(self, params, gradients):\n",
    "        for param, grad in zip(params, gradients):\n",
    "            param -= self.learning_rate * grad\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "        \n",
    "    def update(self, params, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(param) for param in params]\n",
    "            self.v = [np.zeros_like(param) for param in params]\n",
    "        \n",
    "        self.t += 1\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradients[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (gradients[i] ** 2)\n",
    "            \n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            params[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            Conv2D(filters=64, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            Conv2D(filters=64, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            MaxPooling2D(pool_size=(2, 2), stride=2),\n",
    "            Conv2D(filters=128, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            Conv2D(filters=128, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            MaxPooling2D(pool_size=(2, 2), stride=2),\n",
    "            Dropout(dropout_rate=0.5),\n",
    "            Flatten(),\n",
    "            Dense(input_units=6272, units=10, activation='softmax')\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, d_output, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_output = layer.backward(d_output, learning_rate)\n",
    "        return d_output\n",
    "\n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'weights'):\n",
    "                params.append(layer.weights)\n",
    "                params.append(layer.bias)\n",
    "        return params\n",
    "\n",
    "    def get_gradients(self):\n",
    "        gradients = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'grad_weights'):\n",
    "                gradients.append(layer.grad_weights)\n",
    "                gradients.append(layer.grad_bias)\n",
    "        return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, x_test, y_test, batch_size=32):\n",
    "    correct_num, tot_num = 0, 0\n",
    "    for i in range(0, len(x_test), batch_size):\n",
    "        x_batch = x_test[i: i+batch_size]\n",
    "        y_batch = y_test[i: i+batch_size]\n",
    "\n",
    "        predictions = model.forward(x_batch)\n",
    "        predictions_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(y_batch, axis=1)\n",
    "\n",
    "        correct_num += np.sum(predictions_classes == true_classes)\n",
    "        tot_num += batch_size\n",
    "    \n",
    "    accuracy = correct_num / tot_num\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, x_test, y_test, optimizer, batch_size=32, epochs=10):\n",
    "    num_samples = x_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    losses, acces = [], []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = (batch + 1) * batch_size\n",
    "            \n",
    "            x_batch = x_train[start_idx:end_idx]\n",
    "            y_batch = y_train[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model.forward(x_batch)\n",
    "            loss = categorical_crossentropy(predictions, y_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            grad_output = predictions - y_batch\n",
    "            model.backward(grad_output, optimizer.learning_rate)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.update(model.get_params(), model.get_gradients())\n",
    "            \n",
    "            epoch_loss += loss\n",
    "        accuracy = eval(model, x_test, y_test, batch_size)\n",
    "        epoch_loss /= num_batches\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss: .4f}, Accuracy: {accuracy * 100: .2f} %\")\n",
    "\n",
    "        losses.append(epoch_loss)\n",
    "        acces.append(accuracy * 100)\n",
    "\n",
    "    return losses, acces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [18:50<5:58:03, 1130.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss:  2.8767, Accuracy:  57.62 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [37:41<5:39:10, 1130.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss:  0.6957, Accuracy:  71.09 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [56:31<5:20:21, 1130.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss:  0.5944, Accuracy:  73.63 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:15:21<5:01:26, 1130.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss:  0.3902, Accuracy:  77.73 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:34:15<4:42:51, 1131.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss:  0.3011, Accuracy:  81.45 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [1:53:09<4:24:12, 1132.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss:  0.2452, Accuracy:  84.57 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:12:03<4:05:30, 1133.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss:  0.1885, Accuracy:  86.91 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:30:57<3:46:38, 1133.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss:  0.1835, Accuracy:  88.09 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [2:49:50<3:27:45, 1133.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss:  0.1717, Accuracy:  88.09 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:08:43<3:08:52, 1133.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss:  0.1241, Accuracy:  87.89 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:27:37<2:50:00, 1133.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss:  0.0976, Accuracy:  84.77 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [3:46:29<2:31:04, 1133.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss:  0.0830, Accuracy:  88.09 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:05:23<2:12:12, 1133.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss:  0.0753, Accuracy:  88.28 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:24:14<1:53:15, 1132.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss:  0.0708, Accuracy:  88.28 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [4:43:06<1:34:21, 1132.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss:  0.0564, Accuracy:  88.09 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:01:57<1:15:27, 1131.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss:  0.0415, Accuracy:  88.48 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:20:50<56:37, 1132.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss:  0.0672, Accuracy:  90.62 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [5:39:41<37:43, 1131.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss:  0.0587, Accuracy:  89.26 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [5:58:29<18:50, 1130.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss:  0.0296, Accuracy:  90.62 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:17:20<00:00, 1132.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss:  0.0328, Accuracy:  90.62 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import ticker\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据预处理和加载\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),|\n",
    "#      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='../dataset/cifar10', train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='../dataset/cifar10', train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为张量\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 标准化图像数据\n",
    "])\n",
    "\n",
    "subset_train_indices = list(range(500)) # 只取前500个样本\n",
    "subset_test_indices = list(range(500))\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='../dataset/mnist', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset = Subset(trainset, subset_train_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='../dataset/mnist', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testset = Subset(testset, subset_test_indices)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# 转换训练数据为numpy格式\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    for i in range(len(images)):\n",
    "        train_data.append(images[i].numpy())\n",
    "        train_labels.append(np.eye(10)[labels[i]])\n",
    "\n",
    "x_train = np.array(train_data)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# 转换测试数据为numpy格式\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for images, labels in testloader:\n",
    "    for i in range(len(images)):\n",
    "        test_data.append(images[i].numpy())\n",
    "        test_labels.append(np.eye(10)[labels[i]])\n",
    "\n",
    "x_test = np.array(test_data)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "model = VGG()\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "losses, acces = train(model, x_train, y_train, x_test, y_test, optimizer, batch_size=32, epochs=20)\n",
    "\n",
    "# losses = [1.5, 2.5, 3.5, 4.5, 5.5, 30.78, 45.69]\n",
    "# acces = [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axes[0].plot(range(len(acces)), acces, c='r') \n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "axes[0].set_ylabel(\"Test Accuracy(%)\")\n",
    "axes[0].set_title(\"MNIST\")\n",
    "axes[0].yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "axes[0].xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "axes[1].plot(range(len(losses)), losses, c='b')\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "axes[1].set_ylabel(\"Test Loss\")\n",
    "axes[1].set_title(\"MNIST\")\n",
    "axes[1].yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "axes[1].xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
