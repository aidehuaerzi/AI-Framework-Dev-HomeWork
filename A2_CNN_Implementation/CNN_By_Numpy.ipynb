{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义激活函数\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数的导数\n",
    "def relu_derivative(x):\n",
    "    return x > 0\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def cross_entropy_loss(pred, label):\n",
    "    return -np.log(pred[label])\n",
    "\n",
    "def cross_entropy_derivative(pred, label):\n",
    "    grad = pred.copy()\n",
    "    grad[label] -= 1\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, filter_size, input_depth):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size, input_depth) * 0.1\n",
    "        self.biases = np.zeros(num_filters)\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        h, w, d = image.shape\n",
    "        for i in range(h - self.filter_size + 1):\n",
    "            for j in range(w - self.filter_size + 1):\n",
    "                im_region = image[i:(i + self.filter_size), j:(j + self.filter_size)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        h, w, d = input.shape\n",
    "        out_height = h - self.filter_size + 1\n",
    "        out_width = w - self.filter_size + 1\n",
    "        \n",
    "        if out_height <= 0 or out_width <= 0:\n",
    "            raise ValueError(\"Invalid output dimension. Check filter size and input shape.\")\n",
    "        \n",
    "        output = np.zeros((out_height, out_width, self.num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.sum(im_region * self.filters, axis=(1, 2, 3)) + self.biases\n",
    "\n",
    "        return relu(output)\n",
    "\n",
    "    def backward(self, d_L_d_out, learn_rate):\n",
    "        d_L_d_filters = np.zeros(self.filters.shape)\n",
    "        d_L_d_biases = np.zeros(self.biases.shape)\n",
    "        h, w, d = self.last_input.shape\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            for f in range(self.num_filters):\n",
    "                d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "                d_L_d_biases[f] += d_L_d_out[i, j, f]\n",
    "\n",
    "        self.filters -= learn_rate * d_L_d_filters\n",
    "        self.biases -= learn_rate * d_L_d_biases\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义池化层\n",
    "class MaxPoolLayer:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        h, w, d = image.shape\n",
    "        new_h = h // self.pool_size\n",
    "        new_w = w // self.pool_size\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * self.pool_size):(i * self.pool_size + self.pool_size),\n",
    "                                  (j * self.pool_size):(j * self.pool_size + self.pool_size)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        h, w, d = input.shape\n",
    "        output = np.zeros((h // self.pool_size, w // self.pool_size, d))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_L_d_out):\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        if im_region[i2, j2, f2] == amax[f2]:\n",
    "                            d_L_d_input[i * self.pool_size + i2, j * self.pool_size + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "        return d_L_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义全连接层\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_len, nodes):\n",
    "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "        self.biases = np.zeros(nodes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        input_len, nodes = self.weights.shape\n",
    "        totals = np.dot(input, self.weights) + self.biases\n",
    "        return relu(totals)\n",
    "\n",
    "    def backward(self, d_L_d_out, learn_rate):\n",
    "        d_L_d_w = np.dot(self.last_input[np.newaxis].T, d_L_d_out[np.newaxis] * relu_derivative(d_L_d_out))\n",
    "        d_L_d_b = d_L_d_out\n",
    "        d_L_d_input = np.dot(d_L_d_out, self.weights.T).reshape(self.last_input_shape)\n",
    "        self.weights -= learn_rate * d_L_d_w\n",
    "        self.biases -= learn_rate * d_L_d_b\n",
    "        return d_L_d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义BN层\n",
    "class BatchNormLayer:\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.zeros(num_features)\n",
    "\n",
    "    def forward(self, input, training=True):\n",
    "        if training:\n",
    "            mean = np.mean(input, axis=0)\n",
    "            var = np.var(input, axis=0)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        self.input_normalized = (input - mean) / np.sqrt(var + self.epsilon)\n",
    "        return self.gamma * self.input_normalized + self.beta\n",
    "\n",
    "    def backward(self, d_L_d_out, learn_rate):\n",
    "        N, D = d_L_d_out.shape\n",
    "        self.gamma -= learn_rate * np.sum(d_L_d_out * self.input_normalized, axis=0)\n",
    "        self.beta -= learn_rate * np.sum(d_L_d_out, axis=0)\n",
    "        d_input_normalized = d_L_d_out * self.gamma\n",
    "        d_var = np.sum(d_input_normalized * (self.last_input - self.mean) * -0.5 * (self.var + self.epsilon)**(-1.5), axis=0)\n",
    "        d_mean = np.sum(d_input_normalized * -1 / np.sqrt(self.var + self.epsilon), axis=0) + d_var * np.sum(-2 * (self.last_input - self.mean), axis=0) / N\n",
    "        return d_input_normalized / np.sqrt(self.var + self.epsilon) + d_var * 2 * (self.last_input - self.mean) / N + d_mean / N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Dropout层\n",
    "class DropoutLayer:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, input, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.dropout_rate, size=input.shape)\n",
    "            return input * self.mask\n",
    "        else:\n",
    "            return input * (1 - self.dropout_rate)\n",
    "\n",
    "    def backward(self, d_L_d_out):\n",
    "        return d_L_d_out * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for param, grad in zip(params, grads):\n",
    "            param -= self.lr * grad\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if not self.m:\n",
    "            self.m = [np.zeros_like(param) for param in params]\n",
    "            self.v = [np.zeros_like(param) for param in params]\n",
    "\n",
    "        self.t += 1\n",
    "        lr_t = self.lr * (np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t))\n",
    "\n",
    "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "class CNN:\n",
    "    def __init__(self, layers):\n",
    "        # 初始化网络结构\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        # 前向传播\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad, learn_rate):\n",
    "        # 反向传播\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义VGG网络结构\n",
    "class VGG:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            ConvLayer(64, 3, 32),  # input depth = 3 (RGB image)\n",
    "            ConvLayer(64, 3, 64),\n",
    "            MaxPoolLayer(2),\n",
    "            ConvLayer(128, 3, 64),\n",
    "            ConvLayer(128, 3, 128),\n",
    "            MaxPoolLayer(2),\n",
    "            ConvLayer(256, 3, 128),\n",
    "            ConvLayer(256, 3, 256),\n",
    "            ConvLayer(256, 3, 256),\n",
    "            MaxPoolLayer(2),\n",
    "            ConvLayer(512, 3, 256),\n",
    "            ConvLayer(512, 3, 512),\n",
    "            ConvLayer(512, 3, 512),\n",
    "            MaxPoolLayer(2),\n",
    "            ConvLayer(512, 3, 512),\n",
    "            ConvLayer(512, 3, 512),\n",
    "            ConvLayer(512, 3, 512),\n",
    "            MaxPoolLayer(2),\n",
    "            DenseLayer(512 * 1 * 1, 4096),  # Adjust according to final output size\n",
    "            DenseLayer(4096, 4096),\n",
    "            DenseLayer(4096, 10)  # 10 classes for CIFAR-10\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad, learn_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, learn_rate)\n",
    "        return grad\n",
    "\n",
    "    def train(self, x, y, learn_rate=0.001, epochs=1, optimizer=None, batch_size=32):\n",
    "        if optimizer is None:\n",
    "            optimizer = SGD(lr=learn_rate)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, len(x), batch_size):\n",
    "                x_batch = x[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = np.array([self.forward(xi) for xi in x_batch])\n",
    "\n",
    "                # Compute loss and gradient\n",
    "                loss = np.mean([cross_entropy_loss(output, yi) for output, yi in zip(outputs, y_batch)])\n",
    "                grads = np.array([cross_entropy_derivative(output, yi) for output, yi in zip(outputs, y_batch)])\n",
    "\n",
    "                # Backward pass\n",
    "                for grad in grads:\n",
    "                    self.backward(grad, learn_rate)\n",
    "\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, Batch {i // batch_size + 1}/{len(x) // batch_size + 1}, Loss: {loss:.4f}')\n",
    "\n",
    "# Loss function and its derivative\n",
    "def cross_entropy_loss(predictions, labels):\n",
    "    return -np.sum(labels * np.log(predictions + 1e-8))\n",
    "\n",
    "def cross_entropy_derivative(predictions, labels):\n",
    "    return predictions - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3,3) (64,3,3,32) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[222], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m vgg \u001b[38;5;241m=\u001b[39m VGG()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 训练网络\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mvgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[221], line 49\u001b[0m, in \u001b[0;36mVGG.train\u001b[0;34m(self, x, y, learn_rate, epochs, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(xi) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m x_batch])\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Compute loss and gradient\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([cross_entropy_loss(output, yi) \u001b[38;5;28;01mfor\u001b[39;00m output, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, y_batch)])\n",
      "Cell \u001b[0;32mIn[221], line 49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m x_batch])\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Compute loss and gradient\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([cross_entropy_loss(output, yi) \u001b[38;5;28;01mfor\u001b[39;00m output, yi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, y_batch)])\n",
      "Cell \u001b[0;32mIn[221], line 31\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[214], line 27\u001b[0m, in \u001b[0;36mConvLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     24\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((out_height, out_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_filters))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m im_region, i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate_regions(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     output[i, j] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mim_region\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m relu(output)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3,3) (64,3,3,32) "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 数据预处理和加载\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../dataset/cifar10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../dataset/cifar10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# 转换数据为numpy格式\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    for i in range(len(images)):\n",
    "        train_data.append(torch.transpose(images[i], 0, 2).numpy())\n",
    "        train_labels.append(np.eye(10)[labels[i]])\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "\n",
    "# 实例化VGG-16网络\n",
    "vgg = VGG()\n",
    "\n",
    "# 训练网络\n",
    "vgg.train(train_data, train_labels, learn_rate=0.001, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "((array([[-1.45429954,  4.04258727, -1.40327739, ..., -0.        ,\n",
      "        -1.29217646,  0.02903511],\n",
      "       [-0.        , -0.        ,  0.        , ..., -0.        ,\n",
      "        -1.19897518, -0.84347416],\n",
      "       [ 0.        ,  5.06377975, -1.29259632, ...,  0.46079516,\n",
      "        -0.        , -1.39598981],\n",
      "       ...,\n",
      "       [-1.45429954, -1.3688808 , -0.        , ..., -1.4102685 ,\n",
      "        -1.29217646, -1.39598981],\n",
      "       [-1.24962541, -0.        , -0.        , ..., -1.4070241 ,\n",
      "        -1.29217646, -0.        ],\n",
      "       [-1.30292505,  0.59011771, -0.        , ...,  0.        ,\n",
      "        -1.29217646, -0.        ]]), array([[ 0.06577336, -0.01200863,  0.0531466 , ..., -0.02077525,\n",
      "         0.01268506,  0.06442468],\n",
      "       [-0.03896336, -0.0441773 ,  0.15262379, ..., -0.11071828,\n",
      "         0.08655519, -0.02646854],\n",
      "       [-0.02948666, -0.02868987, -0.0902056 , ..., -0.01340982,\n",
      "        -0.07400051,  0.02142804],\n",
      "       ...,\n",
      "       [-0.12149115, -0.01845867, -0.038126  , ...,  0.15099   ,\n",
      "         0.1107986 ,  0.05776026],\n",
      "       [-0.05168205,  0.18466091, -0.03465728, ..., -0.10148439,\n",
      "        -0.00372352,  0.05153101],\n",
      "       [ 0.03453977, -0.03199141,  0.09631723, ...,  0.05668394,\n",
      "        -0.00354521, -0.02364286]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[ 0.0555544 ,  1.57171465, -2.25791998, ...,  6.35422346,\n",
      "        -2.05440454, -1.14449904],\n",
      "       [ 0.64576619, -0.03675653,  2.62172543, ...,  3.14304534,\n",
      "         1.10253384, -0.25225465],\n",
      "       [-2.20016485, -2.17086587,  2.00890965, ..., -0.79485837,\n",
      "        -0.0853359 ,  2.89654972],\n",
      "       ...,\n",
      "       [-2.27127492, -1.18839401,  3.91925525, ..., -2.24537758,\n",
      "        -0.13693224,  0.22746166],\n",
      "       [ 4.17778209,  0.40201725, -0.52169961, ...,  2.4190825 ,\n",
      "        -2.47089809, -2.49154141],\n",
      "       [ 2.30484481,  1.16841185, -1.19953122, ..., -0.59911306,\n",
      "        -1.22004435, -0.88048085]]))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[197], line 366\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[1;32m    365\u001b[0m model \u001b[38;5;241m=\u001b[39m CNNModel(layer_dims, activations, dropout_probs, batchnorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 366\u001b[0m costs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    369\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test\u001b[38;5;241m.\u001b[39mreshape(x_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT, y_test\u001b[38;5;241m.\u001b[39mT)\n",
      "Cell \u001b[0;32mIn[197], line 277\u001b[0m, in \u001b[0;36mCNNModel.fit\u001b[0;34m(self, X, Y, learning_rate, epochs, batch_size, optimizer, print_cost)\u001b[0m\n\u001b[1;32m    275\u001b[0m AL, caches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_propagation(minibatch_X)\n\u001b[1;32m    276\u001b[0m minibatch_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_cost(AL, minibatch_Y)\n\u001b[0;32m--> 277\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatch_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m update_parameters_with_adam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters, grads, learning_rate)\n",
      "Cell \u001b[0;32mIn[197], line 250\u001b[0m, in \u001b[0;36mCNNModel.backward_propagation\u001b[0;34m(self, AL, Y, caches)\u001b[0m\n\u001b[1;32m    248\u001b[0m cache, dropout_cache \u001b[38;5;241m=\u001b[39m current_cache\n\u001b[1;32m    249\u001b[0m bn_cache, activation_cache \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m--> 250\u001b[0m dA_prev_bn, dgamma, dbeta \u001b[38;5;241m=\u001b[39m \u001b[43mbatchnorm_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbn_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m)] \u001b[38;5;241m=\u001b[39m dA_prev_bn\n\u001b[1;32m    252\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m dgamma\n",
      "Cell \u001b[0;32mIn[197], line 105\u001b[0m, in \u001b[0;36mbatchnorm_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatchnorm_backward\u001b[39m(dout, cache):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cache)\n\u001b[0;32m--> 105\u001b[0m     Z, Z_norm, gamma, beta, mu, var, eps \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m    106\u001b[0m     m \u001b[38;5;241m=\u001b[39m Z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    108\u001b[0m     dbeta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dout, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 数据预处理和加载\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../dataset/cifar10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../dataset/cifar10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# 转换数据为numpy格式\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    for i in range(len(images)):\n",
    "        train_data.append(images[i].numpy())\n",
    "        train_labels.append(np.eye(10)[labels[i]])\n",
    "\n",
    "x_train = np.array(train_data)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    m = labels.shape[0]\n",
    "    log_probs = -np.log(logits[np.arange(m), np.argmax(labels, axis=1)])\n",
    "    loss = np.sum(log_probs) / m\n",
    "    return loss\n",
    "\n",
    "def softmax_backward(dout, cache):\n",
    "    Z = cache\n",
    "    s = np.exp(Z) / np.sum(np.exp(Z), axis=-1, keepdims=True)\n",
    "    dZ = dout * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    dZ = np.array(dout, copy=True)\n",
    "    dZ[cache <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dout, cache):\n",
    "    s = cache\n",
    "    dZ = dout * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "def tanh_backward(dout, cache):\n",
    "    t = cache\n",
    "    dZ = dout * (1 - np.square(t))\n",
    "    return dZ\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def batchnorm_forward(Z, gamma, beta, eps=1e-5):\n",
    "    mu = np.mean(Z, axis=0, keepdims=True)\n",
    "    var = np.var(Z, axis=0, keepdims=True)\n",
    "    Z_norm = (Z - mu) / np.sqrt(var + eps)\n",
    "    out = gamma * Z_norm + beta\n",
    "    cache = (Z, Z_norm, gamma, beta, mu, var, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "    print(cache)\n",
    "    Z, Z_norm, gamma, beta, mu, var, eps = cache\n",
    "    m = Z.shape[0]\n",
    "    \n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgamma = np.sum(dout * Z_norm, axis=0)\n",
    "    dZ_norm = dout * gamma\n",
    "    \n",
    "    divar = np.sum(dZ_norm * (Z - mu), axis=0)\n",
    "    dvar = -0.5 * np.sum(dZ_norm * (Z - mu) * np.power(var + eps, -1.5), axis=0)\n",
    "    dmu = np.sum(dZ_norm * (-1 / np.sqrt(var + eps)), axis=0) + dvar * np.mean(-2 * (Z - mu), axis=0)\n",
    "    \n",
    "    dZ = dZ_norm / np.sqrt(var + eps) + dvar * 2 * (Z - mu) / m + dmu / m\n",
    "    \n",
    "    return dZ, dgamma, dbeta\n",
    "\n",
    "def dropout_forward(A, dropout_prob=0.5, mode='train'):\n",
    "    if mode == 'train':\n",
    "        mask = (np.random.rand(*A.shape) < (1 - dropout_prob)) / (1 - dropout_prob)\n",
    "        A = A * mask\n",
    "        cache = mask\n",
    "    else:\n",
    "        cache = None\n",
    "    return A, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    mask = cache\n",
    "    dA = dout * mask\n",
    "    return dA\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z), Z\n",
    "    elif activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z), Z\n",
    "    elif activation == \"tanh\":\n",
    "        A, activation_cache = tanh(Z), Z\n",
    "    elif activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z), Z\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "# CNN Model class\n",
    "class CNNModel:\n",
    "    def __init__(self, layer_dims, activations, dropout_probs, batchnorm=False):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.activations = activations\n",
    "        self.dropout_probs = dropout_probs\n",
    "        self.batchnorm = batchnorm\n",
    "        self.parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    def forward_propagation(self, X, mode='train'):\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(self.layer_dims) - 1\n",
    "        \n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            W = self.parameters['W' + str(l)]\n",
    "            b = self.parameters['b' + str(l)]\n",
    "            activation = self.activations[l-1]\n",
    "            \n",
    "            A, cache = linear_activation_forward(A_prev, W, b, activation)\n",
    "            \n",
    "            if self.batchnorm:\n",
    "                gamma = np.ones((self.layer_dims[l], 1))\n",
    "                beta = np.zeros((self.layer_dims[l], 1))\n",
    "                A, bn_cache = batchnorm_forward(A, gamma, beta)\n",
    "                cache = (cache, bn_cache)\n",
    "            \n",
    "            if mode == 'train' and self.dropout_probs[l-1] > 0:\n",
    "                A, dropout_cache = dropout_forward(A, self.dropout_probs[l-1], mode)\n",
    "                cache = (cache, dropout_cache)\n",
    "            \n",
    "            caches.append(cache)\n",
    "        \n",
    "        # Output layer\n",
    "        W = self.parameters['W' + str(L)]\n",
    "        b = self.parameters['b' + str(L)]\n",
    "        AL, cache = linear_activation_forward(A, W, b, \"softmax\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "        return AL, caches\n",
    "    \n",
    "    def compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        cost = cross_entropy_loss(AL, Y)\n",
    "        return cost\n",
    "    \n",
    "    def backward_propagation(self, AL, Y, caches):\n",
    "        grads = {}\n",
    "        L = len(self.parameters) // 2\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "        \n",
    "        # Compute gradients\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        current_cache = caches[-1]\n",
    "        grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "        \n",
    "        for l in reversed(range(L-1)):\n",
    "            current_cache = caches[l]\n",
    "            \n",
    "            if self.batchnorm:\n",
    "                cache, dropout_cache = current_cache\n",
    "                bn_cache, activation_cache = cache\n",
    "                dA_prev_bn, dgamma, dbeta = batchnorm_backward(grads[\"dA\" + str(l + 2)], bn_cache)\n",
    "                grads[\"dA\" + str(l + 2)] = dA_prev_bn\n",
    "                grads[\"dgamma\" + str(l + 1)] = dgamma\n",
    "                grads[\"dbeta\" + str(l + 1)] = dbeta\n",
    "                dA_prev_dropout = dropout_backward(grads[\"dA\" + str(l + 2)], dropout_cache)\n",
    "                grads[\"dA\" + str(l + 2)] = dA_prev_dropout\n",
    "            else:\n",
    "                grads[\"dA\" + str(l + 2)], grads[\"dW\" + str(l + 2)], grads[\"db\" + str(l + 2)] = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, self.activations[l])\n",
    "        \n",
    "        # First layer gradients\n",
    "        current_cache = caches[0]\n",
    "        grads[\"dA1\"], grads[\"dW1\"], grads[\"db1\"] = linear_activation_backward(grads[\"dA2\"], current_cache, self.activations[0])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.01, epochs=10, batch_size=64, optimizer='adam', print_cost=True):\n",
    "        m = X.shape[1]\n",
    "        costs = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            minibatches = random_mini_batches(X, Y, batch_size)\n",
    "            minibatch_cost = 0\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                AL, caches = self.forward_propagation(minibatch_X)\n",
    "                minibatch_cost += self.compute_cost(AL, minibatch_Y)\n",
    "                grads = self.backward_propagation(AL, minibatch_Y, caches)\n",
    "                \n",
    "                if optimizer == \"adam\":\n",
    "                    self.parameters = update_parameters_with_adam(self.parameters, grads, learning_rate)\n",
    "                elif optimizer == \"gd\":\n",
    "                    self.parameters = update_parameters_with_gd(self.parameters, grads, learning_rate)\n",
    "                    \n",
    "            epoch_cost = minibatch_cost / m\n",
    "            \n",
    "            if print_cost and epoch % 10 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        return costs\n",
    "\n",
    "    def predict(self, X):\n",
    "        AL, _ = self.forward_propagation(X, mode='test')\n",
    "        predictions = np.argmax(AL, axis=0)\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == np.argmax(Y, axis=1))\n",
    "        return accuracy * 100\n",
    "    \n",
    "# Utility functions\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size=64):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m))\n",
    "    \n",
    "    # Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = m // mini_batch_size\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
    "    L = len(parameters) // 2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - np.power(beta1, t))\n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - np.power(beta2, t))\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - np.power(beta1, t))\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - np.power(beta2, t))\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Model architecture for VGG-like network\n",
    "layer_dims = [32*32*3, 512, 512, 10]\n",
    "activations = [\"relu\", \"relu\", \"softmax\"]\n",
    "dropout_probs = [0.5, 0.5, 0]\n",
    "\n",
    "# Initialize and train the model\n",
    "model = CNNModel(layer_dims, activations, dropout_probs, batchnorm=True)\n",
    "costs = model.fit(x_train.reshape(x_train.shape[0], -1).T, y_train.T, learning_rate=0.001, epochs=50, batch_size=128, optimizer='adam')\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(x_test.reshape(x_test.shape[0], -1).T, y_test.T)\n",
    "print(f'Test Accuracy: {accuracy}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
