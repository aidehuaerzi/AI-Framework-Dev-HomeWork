{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # 用于显示训练进度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "def softmax_crossentropy(y_pred, y_true):\n",
    "    m = y_pred.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def l2_regularization(weights, lambda_param):\n",
    "    return 0.5 * lambda_param * np.sum(weights**2)\n",
    "\n",
    "def l1_regularization(weights, lambda_param):\n",
    "    return lambda_param * np.sum(np.abs(weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, filter_size, num_channels, stride=1, padding='same', weight_decay=0.0):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.num_channels = num_channels\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size, num_channels) * 0.1\n",
    "        self.weight_decay = weight_decay\n",
    "        self.biases = np.zeros((num_filters, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        batch_size, input_height, input_width, input_channels = inputs.shape\n",
    "        output_height = (input_height - self.filter_size + 2 * (self.padding == 'same')) // self.stride + 1\n",
    "        output_width = (input_width - self.filter_size + 2 * (self.padding == 'same')) // self.stride + 1\n",
    "        self.outputs = np.zeros((batch_size, output_height, output_width, self.num_filters))\n",
    "\n",
    "        padded_inputs = np.pad(inputs, [(0, 0), (self.filter_size // 2, self.filter_size // 2), (self.filter_size // 2, self.filter_size // 2), (0, 0)], mode='constant')\n",
    "\n",
    "        for y in range(output_height):\n",
    "            for x in range(output_width):\n",
    "                for f in range(self.num_filters):\n",
    "                    y_start = y * self.stride\n",
    "                    y_end = y_start + self.filter_size\n",
    "                    x_start = x * self.stride\n",
    "                    x_end = x_start + self.filter_size\n",
    "                    self.outputs[:, y, x, f] = np.sum(padded_inputs[:, y_start:y_end, x_start:x_end, :] * self.filters[f, :, :, :], axis=(1, 2, 3)) + self.biases[f]\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dloss_dout, learning_rate):\n",
    "        batch_size, input_height, input_width, input_channels = self.inputs.shape\n",
    "        dloss_dfilters = np.zeros_like(self.filters)\n",
    "        dloss_dbiases = np.zeros_like(self.biases)\n",
    "        padded_inputs = np.pad(self.inputs, [(0, 0), (self.filter_size // 2, self.filter_size // 2), (self.filter_size // 2, self.filter_size // 2), (0, 0)], mode='constant')\n",
    "        dloss_dinputs = np.zeros_like(padded_inputs)\n",
    "\n",
    "        for y in range(self.outputs.shape[1]):\n",
    "            for x in range(self.outputs.shape[2]):\n",
    "                for f in range(self.num_filters):\n",
    "                    y_start = y * self.stride\n",
    "                    y_end = y_start + self.filter_size\n",
    "                    x_start = x * self.stride\n",
    "                    x_end = x_start + self.filter_size\n",
    "                    dloss_dfilters[f, :, :, :] += np.sum(np.expand_dims(dloss_dout[:, y, x, f], axis=-1) * padded_inputs[:, y_start:y_end, x_start:x_end, :], axis=0)\n",
    "                    dloss_dbiases[f] += np.sum(dloss_dout[:, y, x, f], axis=0)\n",
    "                    dloss_dinputs[:, y_start:y_end, x_start:x_end, :] += np.expand_dims(dloss_dout[:, y, x, f], axis=-1) * self.filters[f, :, :, :]\n",
    "\n",
    "        self.filters -= learning_rate * dloss_dfilters / batch_size\n",
    "        self.biases -= learning_rate * dloss_dbiases / batch_size\n",
    "\n",
    "        dloss_dinputs = dloss_dinputs[:, self.filter_size // 2:-self.filter_size // 2, self.filter_size // 2:-self.filter_size // 2, :]\n",
    "\n",
    "        return dloss_dinputs\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        return 0.5 * self.weight_decay * np.sum(self.filters ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        batch_size, input_height, input_width, input_channels = inputs.shape\n",
    "        output_height = (input_height - self.pool_size) // self.stride + 1\n",
    "        output_width = (input_width - self.pool_size) // self.stride + 1\n",
    "        self.outputs = np.zeros((batch_size, output_height, output_width, input_channels))\n",
    "\n",
    "        for y in range(output_height):\n",
    "            for x in range(output_width):\n",
    "                for c in range(input_channels):\n",
    "                    y_start = y * self.stride\n",
    "                    y_end = y_start + self.pool_size\n",
    "                    x_start = x * self.stride\n",
    "                    x_end = x_start + self.pool_size\n",
    "                    self.outputs[:, y, x, c] = np.max(inputs[:, y_start:y_end, x_start:x_end, c], axis=(1, 2))\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dloss_dout, learning_rate):\n",
    "        batch_size, input_height, input_width, input_channels = self.inputs.shape\n",
    "        dloss_dinputs = np.zeros_like(self.inputs)\n",
    "\n",
    "        for y in range(dloss_dout.shape[1]):\n",
    "            for x in range(dloss_dout.shape[2]):\n",
    "                for c in range(input_channels):\n",
    "                    y_start = y * self.stride\n",
    "                    y_end = y_start + self.pool_size\n",
    "                    x_start = x * self.stride\n",
    "                    x_end = x_start + self.pool_size\n",
    "                    max_pool = np.max(self.inputs[:, y_start:y_end, x_start:x_end, c], axis=(1, 2), keepdims=True)\n",
    "                    mask = (self.inputs[:, y_start:y_end, x_start:x_end, c] == max_pool)\n",
    "                    dloss_dinputs[:, y_start:y_end, x_start:x_end, c] += mask * np.expand_dims(dloss_dout[:, y, x, c], axis=-1)\n",
    "\n",
    "        return dloss_dinputs\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, weight_decay=0.0):\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dloss_dout, learning_rate):\n",
    "        dloss_dweights = np.dot(self.inputs.T, dloss_dout)\n",
    "        dloss_dbiases = np.sum(dloss_dout, axis=0, keepdims=True)\n",
    "        dloss_dinputs = np.dot(dloss_dout, self.weights.T)\n",
    "\n",
    "        self.weights -= learning_rate * dloss_dweights\n",
    "        self.biases -= learning_rate * dloss_dbiases\n",
    "\n",
    "        return dloss_dinputs\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        return 0.5 * self.weight_decay * np.sum(self.weights ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormLayer:\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9, weight_decay=0.0):\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "        self.running_mean = np.zeros((1, num_features))\n",
    "        self.running_var = np.ones((1, num_features))\n",
    "        self.batch_size = None\n",
    "        self.x_normalized = None\n",
    "\n",
    "    def forward(self, inputs, mode='train'):\n",
    "        self.inputs_shape = inputs.shape\n",
    "        if mode == 'train':\n",
    "            self.batch_size, self.num_features = inputs.shape[0], np.prod(inputs.shape[1:])\n",
    "            self.mean = np.mean(inputs, axis=0)\n",
    "            self.variance = np.var(inputs, axis=0)\n",
    "            self.x_normalized = (inputs - self.mean) / np.sqrt(self.variance + self.epsilon)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.variance\n",
    "        else:\n",
    "            self.x_normalized = (inputs - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "\n",
    "        self.outputs = self.gamma * self.x_normalized + self.beta\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dloss_dout, learning_rate):\n",
    "        dgamma = np.sum(dloss_dout * self.x_normalized, axis=0, keepdims=True)\n",
    "        dbeta = np.sum(dloss_dout, axis=0, keepdims=True)\n",
    "        dx_normalized = dloss_dout * self.gamma\n",
    "        dvariance = np.sum(dx_normalized * (self.inputs - self.mean) * -0.5 * np.power(self.variance + self.epsilon, -1.5), axis=0, keepdims=True)\n",
    "        dmean = np.sum(dx_normalized * -1.0 / np.sqrt(self.variance + self.epsilon), axis=0, keepdims=True) + dvariance * np.mean(-2.0 * (self.inputs - self.mean), axis=0, keepdims=True)\n",
    "        dinputs = (dx_normalized * 1.0 / np.sqrt(self.variance + self.epsilon)) + (dvariance * 2.0 * (self.inputs - self.mean) / self.batch_size) + (dmean / self.batch_size)\n",
    "        self.gamma -= learning_rate * dgamma\n",
    "        self.beta -= learning_rate * dbeta\n",
    "        return dinputs.reshape(self.inputs_shape)\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        reg_loss = 0.5 * self.weight_decay * (np.sum(self.gamma ** 2) + np.sum(self.beta ** 2))\n",
    "        return reg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutLayer:\n",
    "    def __init__(self, dropout_prob):\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, inputs, mode='train'):\n",
    "        if mode == 'train':\n",
    "            self.mask = np.random.binomial(1, 1 - self.dropout_prob, size=inputs.shape) / (1 - self.dropout_prob)\n",
    "            self.outputs = inputs * self.mask\n",
    "        else:\n",
    "            self.outputs = inputs\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dloss_dout, learning_rate):\n",
    "        return dloss_dout * self.mask\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "        self.inputs = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, inputs)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-inputs))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(inputs)\n",
    "        elif self.activation == \"softmax\":\n",
    "            return self.softmax(inputs)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported activation function: {self.activation}')\n",
    "        \n",
    "    def softmax(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def backward(self, dloss_dout, learning_rate):\n",
    "        if self.activation == 'relu':\n",
    "            return dloss_dout * (self.inputs > 0)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid_output = 1 / (1 + np.exp(-self.inputs))\n",
    "            return dloss_dout * sigmoid_output * (1 - sigmoid_output)\n",
    "        elif self.activation == 'tanh':\n",
    "            return\n",
    "    def regularization_loss(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.input_shape = inputs.shape\n",
    "        return inputs.reshape(self.input_shape[0], -1)\n",
    "\n",
    "    def backward(self, dloss_dout, learning_rate):\n",
    "        return dloss_dout.reshape(self.input_shape)\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.layers = [\n",
    "            ConvLayer(num_filters=32, filter_size=3, num_channels=input_shape[2]),\n",
    "            BatchNormLayer(num_features=32),\n",
    "            ActivationLayer(activation='relu'),\n",
    "            ConvLayer(num_filters=32, filter_size=3, num_channels=32),\n",
    "            BatchNormLayer(num_features=32),\n",
    "            ActivationLayer(activation='relu'),\n",
    "            MaxPoolLayer(pool_size=2, stride=2),\n",
    "            DropoutLayer(dropout_prob=0.25),\n",
    "            \n",
    "            ConvLayer(num_filters=64, filter_size=3, num_channels=32),\n",
    "            BatchNormLayer(num_features=64),\n",
    "            ActivationLayer(activation='relu'),\n",
    "            ConvLayer(num_filters=64, filter_size=3, num_channels=64),\n",
    "            BatchNormLayer(num_features=64),\n",
    "            ActivationLayer(activation='relu'),\n",
    "            MaxPoolLayer(pool_size=2, stride=2),\n",
    "            DropoutLayer(dropout_prob=0.25),\n",
    "            \n",
    "            FlattenLayer(),\n",
    "            DenseLayer(input_size=64*8*8, output_size=512),\n",
    "            BatchNormLayer(num_features=512),\n",
    "            ActivationLayer(activation='relu'),\n",
    "            DropoutLayer(dropout_prob=0.5),\n",
    "            DenseLayer(input_size=512, output_size=num_classes),  # Output layer\n",
    "            ActivationLayer(activation='softmax')  # Softmax activation for output layer\n",
    "        ]\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, loss_grad, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad, learning_rate)\n",
    "        return loss_grad\n",
    "\n",
    "    def predict(self, X):\n",
    "        logits = self.forward(X)\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.mean(y_pred == np.argmax(y, axis=1))\n",
    "        return accuracy\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        reg_loss = 0.0\n",
    "        for layer in self.layers:\n",
    "            reg_loss += layer.regularization_loss()\n",
    "        return reg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_val, y_val, optimizer='sgd', learning_rate=0.01, epochs=10, batch_size=64, verbose=True):\n",
    "    num_batches = X_train.shape[0] // batch_size\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = SGD()\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam()\n",
    "    elif optimizer == 'adagrad':\n",
    "        optimizer = Adagrad()\n",
    "    elif optimizer == 'gd':\n",
    "        optimizer = GradientDescent()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        for batch in tqdm(range(num_batches), disable=not verbose):\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            logits = model.forward(X_batch)\n",
    "            loss = softmax_crossentropy(logits, y_batch)\n",
    "            reg_loss = model.regularization_loss()\n",
    "            total_loss = loss + reg_loss\n",
    "            epoch_train_loss += total_loss\n",
    "\n",
    "            grad = model.backward(loss_grad=1, learning_rate=learning_rate)\n",
    "            optimizer.update(model.layers)\n",
    "\n",
    "        train_losses.append(epoch_train_loss / num_batches)\n",
    "\n",
    "        # Validation loss\n",
    "        val_logits = model.forward(X_val, mode='test')\n",
    "        val_loss = softmax_crossentropy(val_logits, y_val)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "class SGD:\n",
    "    def update(self, layers):\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'weights'):\n",
    "                layer.weights -= learning_rate * layer.weights_grad\n",
    "                layer.biases -= learning_rate * layer.biases_grad\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layers):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(layer.weights) for layer in layers if hasattr(layer, 'weights')]\n",
    "            self.v = [np.zeros_like(layer.weights) for layer in layers if hasattr(layer, 'weights')]\n",
    "\n",
    "        self.t += 1\n",
    "        for i, layer in enumerate(layers):\n",
    "            if hasattr(layer, 'weights'):\n",
    "                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * layer.weights_grad\n",
    "                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (layer.weights_grad ** 2)\n",
    "                m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "                layer.weights -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "                layer.biases -= self.lr * layer.biases_grad\n",
    "\n",
    "class Adagrad:\n",
    "    def __init__(self, lr=0.01, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.cache = None\n",
    "\n",
    "    def update(self, layers):\n",
    "        if self.cache is None:\n",
    "            self.cache = [np.zeros_like(layer.weights) for layer in layers if hasattr(layer, 'weights')]\n",
    "\n",
    "        for i, layer in enumerate(layers):\n",
    "            if hasattr(layer, 'weights'):\n",
    "                self.cache[i] += layer.weights_grad ** 2\n",
    "                layer.weights -= self.lr * layer.weights_grad / (np.sqrt(self.cache[i]) + self.epsilon)\n",
    "                layer.biases -= self.lr * layer.biases_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/781 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.57265202 0.         0.         ... 0.         0.45247728 0.        ]\n",
      " [0.         0.82837421 0.         ... 0.81093453 0.         0.        ]\n",
      " [0.         0.         0.         ... 1.15720429 0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         1.83063317 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         3.3716495 ]]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m VGGNet(input_shape\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:], num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X_train, y_train, X_val, y_val, optimizer, learning_rate, epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m reg_loss\n\u001b[1;32m     29\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\n\u001b[0;32m---> 31\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mupdate(model\u001b[38;5;241m.\u001b[39mlayers)\n\u001b[1;32m     34\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_train_loss \u001b[38;5;241m/\u001b[39m num_batches)\n",
      "Cell \u001b[0;32mIn[23], line 38\u001b[0m, in \u001b[0;36mVGGNet.backward\u001b[0;34m(self, loss_grad, learning_rate)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss_grad, learning_rate):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 38\u001b[0m         loss_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_grad\n",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m, in \u001b[0;36mDenseLayer.backward\u001b[0;34m(self, dloss_dout, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(dloss_dout)\n\u001b[0;32m---> 15\u001b[0m dloss_dweights \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdloss_dout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m dloss_dbiases \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dloss_dout, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m dloss_dinputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dloss_dout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 数据预处理和加载\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../dataset/cifar10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../dataset/cifar10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# 转换数据为numpy格式\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    for i in range(len(images)):\n",
    "        train_data.append(torch.transpose(images[i], 0, 2).numpy())\n",
    "        train_labels.append(np.eye(10)[labels[i]])\n",
    "\n",
    "X_train = np.array(train_data)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# Create VGG model\n",
    "model = VGGNet(input_shape=X_train.shape[1:], num_classes=10)\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train(model, X_train, y_train, None, None, optimizer='adam', learning_rate=0.001, epochs=10, batch_size=64, verbose=True)\n",
    "\n",
    "# Evaluate on test set\n",
    "# test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
